## 优化目标


$$
w^*, b^* = \arg\min_{w, b} l(X, y, w, b)
$$


若你要继续展开，可以具体说明损失函数的形式，例如在线性回归中，通常使用的是均方误差（MSE）：


$$
l(X, y, w, b) = \frac{1}{n} \sum_{i=1}^{n} (w^\top x_i + b - y_i)^2
$$


这样整个公式就可以写为：


$$
w^*, b^* = \arg\min_{w, b} \frac{1}{n} \sum_{i=1}^{n} (w^\top x_i + b - y_i)^2
$$

“arg” 是数学中的一个缩写，常出现在优化表达式中，比如：


$$
\arg\min_{w,b} l(X, y, w, b)
$$


其中的 `arg` 是 **“argument”** 的缩写，在这里的意思是：**使目标函数取得最小值（或最大值）时所对应的变量值**。

## 小批量随机梯度下降

- Mini-batch Stochastic Gradient Descent (SGD)

- stochastic /stɒˈkæstɪk/ adj.[数] 随机的；猜测的

- $W_{t+1}=W_t-\eta_t \nabla_w f(X,w,y) $ 

### code

```python
def data_iter(batch_size, features, labels):
    num_examples = len(features)
    indices = list(range(num_examples))
    random.shuffle(indices)
    for i in range(0, num_examples, batch_size):
        batch_indices = indices[i:min(i+batch_size,num_examples)]
        batch_indices = torch.tensor(batch_indices)  # 转为 tensor
        yield features[batch_indices], labels[batch_indices]

n, p = features.shape
batch_size = 8
learning_rate = 0.01
num_epochs = 8

w = torch.normal(0, 0.01, size=(p, 1), requires_grad=True)
b = torch.zeros(1, requires_grad=True)

for epoch in range(num_epochs):
    for X, y in data_iter(batch_size, features, labels):
        y_hat = X @ w + b
        loss = ((y_hat - y) ** 2 / 2).mean()  # 均方误差损失
        loss.backward()  # 反向传播计算梯度

        with torch.no_grad():
            for param in [w, b]:
                param.data -= learning_rate * param.grad
                param.grad.zero_()

    print(f'epoch {epoch + 1}, loss {loss.item():.4f}')

```

- `range(n)` : 生成一个从 0 到 n-1 的整数序列，类型是一个 range 对象

