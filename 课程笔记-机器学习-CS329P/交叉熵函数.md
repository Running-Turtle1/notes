交叉熵函数（Cross-Entropy Loss）是机器学习和深度学习中常用的损失函数，尤其在分类问题（如二分类和多分类）中应用广泛。它衡量的是模型预测的概率分布与真实标签分布之间的差异，值越小表示预测越接近真实标签。



### 1. 二分类交叉熵（Binary Cross-Entropy）


假设标签 $y \in \{0,1\}$，模型输出的预测概率为 $\hat{y} \in [0,1]$，交叉熵定义为：


$$
\text{BCE}(y, \hat{y}) = - \big( y \log(\hat{y}) + (1 - y) \log(1 - \hat{y}) \big)
$$


- 当真实标签是1时，损失主要取决于 $-\log(\hat{y})$，模型预测的概率越接近1，损失越小。
- 当真实标签是0时，损失主要取决于 $-\log(1-\hat{y})$，模型预测的概率越接近0，损失越小。


### 2. 多分类交叉熵（Categorical Cross-Entropy）


对于多分类问题，假设类别数为 $C$，真实标签为 one-hot 编码 $y = [y_1, y_2, ..., y_C]$，预测概率为 $\hat{y} = [\hat{y}_1, \hat{y}_2, ..., \hat{y}_C]$，则交叉熵为：


$$
\text{CCE}(y, \hat{y}) = - \sum_{i=1}^C y_i \log(\hat{y}_i)
$$


实际中，只有真实类别对应的 $y_i=1$，其余为0，所以本质上是取该类别对应概率的负对数。